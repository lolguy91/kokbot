'use strict';
var __createBinding =
	(this && this.__createBinding) ||
	(Object.create
		? function (o, m, k, k2) {
				if (k2 === undefined) k2 = k;
				var desc = Object.getOwnPropertyDescriptor(m, k);
				if (
					!desc ||
					('get' in desc
						? !m.__esModule
						: desc.writable || desc.configurable)
				) {
					desc = {
						enumerable: true,
						get: function () {
							return m[k];
						},
					};
				}
				Object.defineProperty(o, k2, desc);
		  }
		: function (o, m, k, k2) {
				if (k2 === undefined) k2 = k;
				o[k2] = m[k];
		  });
var __setModuleDefault =
	(this && this.__setModuleDefault) ||
	(Object.create
		? function (o, v) {
				Object.defineProperty(o, 'default', {
					enumerable: true,
					value: v,
				});
		  }
		: function (o, v) {
				o['default'] = v;
		  });
var __importStar =
	(this && this.__importStar) ||
	function (mod) {
		if (mod && mod.__esModule) return mod;
		var result = {};
		if (mod != null)
			for (var k in mod)
				if (
					k !== 'default' &&
					Object.prototype.hasOwnProperty.call(mod, k)
				)
					__createBinding(result, mod, k);
		__setModuleDefault(result, mod);
		return result;
	};
var __importDefault =
	(this && this.__importDefault) ||
	function (mod) {
		return mod && mod.__esModule ? mod : { default: mod };
	};
Object.defineProperty(exports, '__esModule', { value: true });
exports.store = exports.store_multiple = exports.store_stream = void 0;
const Json = __importStar(require('doge-json'));
const fs_1 = __importDefault(require('fs'));
const path_1 = __importDefault(require('path'));
const ps_std_1 = require('ps-std');
const serial_async_io_1 = __importDefault(require('serial-async-io'));
const utils_1 = require('./utils');
const stat = (0, ps_std_1.cacheFn)(serial_async_io_1.default.stat);
const streamer = new ps_std_1.ProceduralQueue((filename) => {
	return new Promise((resolve) => {
		const chunks = new Array();
		const stream = fs_1.default.createReadStream(filename);
		stream.on('data', (chunk) =>
			chunks.push((0, utils_1.store_buffer)(chunk))
		);
		stream.on('end', () => resolve(Promise.all(chunks)));
	});
});
async function store_stream(filename) {
	const { output } = await streamer.await(filename);
	return output;
}
exports.store_stream = store_stream;
async function store_multiple(filenames) {
	const promises = new Array();
	for (const filename of filenames) {
		promises.push(await store(filename));
	}
	return Promise.all(promises);
}
exports.store_multiple = store_multiple;
async function store(filename) {
	try {
		const stats = await stat(filename);
		if (!stats) {
			throw `Cannot access ${filename}`;
		} else if (stats.isDirectory()) {
			const list = await fs_1.default.promises.readdir(filename);
			const files = list.map((child) =>
				path_1.default.resolve(filename, child)
			);
			const children = await store_multiple(files);
			const record = {
				name: path_1.default.basename(filename),
				type: 'directory',
				children: children,
			};
			return (0, utils_1.store_object)(record);
		} else if (stats.isFile()) {
			if (stats.size > stats.blksize) {
				const parts = await store_stream(filename);
				const entry = {
					name: path_1.default.basename(filename),
					type: 'longfile',
					parts,
					size: stats.size,
				};
				return (0, utils_1.store_object)(entry);
			} else {
				const entry = {
					name: path_1.default.basename(filename),
					type: 'file',
					hash: await (0, utils_1.store_buffer)(
						await serial_async_io_1.default.read(filename)
					),
					size: stats.size,
				};
				return (0, utils_1.store_object)(entry);
			}
		} else {
			throw Json.encode({ stats });
		}
	} catch (error) {
		const buffer = Buffer.from(String(error));
		const hash = await (0, utils_1.store_buffer)(buffer);
		const size = buffer.length;
		const record = {
			name: path_1.default.basename(filename),
			type: 'file',
			hash,
			size,
		};
		return (0, utils_1.store_object)(record);
	}
}
exports.store = store;
